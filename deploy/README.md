# Solcae

## Overview


## Prerequisites

- AWS SAM CLI, `aws-sam-cli`
- Python 3.10
- AWS account with access to Secrets Manager
- OPENAI api key

## Setup

1. **Save Secrets**
- **Production**
  - Store your OpenAI API key in AWS Secrets Manager under the name `OPENAI_API_KEY`. Keep it under `openai` in `us-west-2` region.

- **Development**
  - Keep OPENAI_API_KEY in `.env` file
  - For testing locally using `sam local invoke`, keep the secrets in .env.json 
    ```
    {"Parameters": 
      {"OPENAI_API_KEY":"abcd"}
    }
    ```

2. **Install Dependencies**: 
   ```bash
   pip install -r src/requirements.txt
   ```

## Build and Test Locally

  - **Build the Project**:
    ```bash
    sam build
    ```

  - **Invoke Locally**:
    ```bash
    sam build && sam local invoke --event event.json --env-vars .env.json
    ```

  - **Run unit tests**
    - Keep OPENAI_API_KEY in dotenv file
    - Run `python -m unittest tests/test_solace.py`

## Deployment

  - **Deploy the Application**:
    ```bash
    sam deploy
    ```

  - **Sync code**
    ```bash
    sam sync --watch --stack-name convagent
    ```

## File Structure

- `src/solace.py`: Main application script.
- `src/requirements.txt`: Python dependencies.
- `event.json`: Sample event data for local testing.
- `README.md`
- `samconfig.toml`: This file contains configuration settings for the AWS SAM CLI. By using `samconfig.toml`, you can avoid specifying these parameters manually each time you run a `sam` command.
- `template.yaml`: AWS SAM template for defining the serverless application.
- `test_lamdba_api.py`: Test script for the lambda api.
- `.env`: Only for local environment.
- `.env.json`: Only for local environment.


## **Assignment Instructions**
Your task is to create an API endpoint within AWS infrastructure such that it is operating as a wrapper function for an LLM endpoint. This LLM endpoint could be ChatGPT, Groq, Bedrock, or some other LLM model service. As part of the Payload parameter for a request, it is required to have a parameter “messages” that is effectively a list of messages that are used as the primary input for the LLM model. The messages object should take the standard format as found in OpenAI endpoints, as this is the most commonly adopted format.

The wrapper endpoint must return a response that includes the chat message that is generated by the underlying LLM. All code deployed to AWS must be in Python, and shared with the reviewing team. 

Finally, the endpoint must be callable using only the requests library in Python. To validate your work, be certain to share a python code snippet that can be copied and pasted - without modification - and ran in any simple python environment to retrieve an LLM response from the wrapper endpoint that you create. 

Additionally, using GenAI tools IS allowed to assist you in this process. However, be certain to understand everything you have done - and all aspects of your code - because you will be asked about these details in future conversations.